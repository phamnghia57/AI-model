{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f8ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\datasets--cnn_dailymail. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 287113/287113 [00:02<00:00, 125986.61 examples/s]\n",
      "Generating validation split: 100%|██████████| 13368/13368 [00:00<00:00, 118682.09 examples/s]\n",
      "Generating test split: 100%|██████████| 11490/11490 [00:00<00:00, 126589.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load CNN/DailyMail\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Lấy subset nhỏ để train nhanh\n",
    "train_data = dataset[\"train\"].select(range(5000))\n",
    "val_data   = dataset[\"validation\"].select(range(500))\n",
    "\n",
    "# Xây tokenizer đơn giản (từ vocab tự động)\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts, max_size=10000):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(t.lower().split())\n",
    "    vocab = {\"<pad>\":0,\"<sos>\":1,\"<eos>\":2,\"<unk>\":3}\n",
    "    for word, _ in counter.most_common(max_size-len(vocab)):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_vocab(train_data[\"article\"])\n",
    "tgt_vocab = build_vocab(train_data[\"highlights\"])\n",
    "inv_tgt_vocab = {i:w for w,i in tgt_vocab.items()}\n",
    "\n",
    "def encode(text, vocab, max_len=100, add_sos=False, add_eos=True):\n",
    "    ids = []\n",
    "    if add_sos: ids.append(vocab[\"<sos>\"])\n",
    "    for w in text.lower().split():\n",
    "        ids.append(vocab.get(w, vocab[\"<unk>\"]))\n",
    "        if len(ids) >= max_len: break\n",
    "    if add_eos: ids.append(vocab[\"<eos>\"])\n",
    "    ids = ids[:max_len]\n",
    "    ids += [vocab[\"<pad>\"]] * (max_len-len(ids))\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d67afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, articles, summaries, src_vocab, tgt_vocab, max_src=100, max_tgt=30):\n",
    "        self.src = [encode(a, src_vocab, max_src) for a in articles]\n",
    "        self.tgt = [encode(s, tgt_vocab, max_tgt, add_sos=True) for s in summaries]\n",
    "\n",
    "    def __len__(self): return len(self.src)\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.src[i]), torch.tensor(self.tgt[i])\n",
    "\n",
    "train_dataset = CNNDataset(train_data[\"article\"], train_data[\"highlights\"], src_vocab, tgt_vocab)\n",
    "val_dataset   = CNNDataset(val_data[\"article\"], val_data[\"highlights\"], src_vocab, tgt_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668fc7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim*2, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.permute(1,0,2).repeat(1,src_len,1)  # [batch,src_len,hid_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2) # [batch,src_len]\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, attention):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(hid_dim+emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim*2+emb_dim, output_dim)\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        attn_weights = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    "        context = attn_weights.bmm(encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        output = torch.cat((output.squeeze(1), context.squeeze(1), embedded.squeeze(1)), dim=1)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.fc_out.out_features).to(src.device)\n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "        input_tok = tgt[:,0]\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden = self.decoder(input_tok, hidden, enc_outputs)\n",
    "            outputs[:,t,:] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input_tok = tgt[:,t] if teacher_force else output.argmax(1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa323ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=7.0416\n",
      "Epoch 2, loss=6.5664\n",
      "Epoch 3, loss=6.1883\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(tgt_vocab)\n",
    "EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)\n",
    "attn = Attention(HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, attn)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<pad>\"])\n",
    "\n",
    "for epoch in range(3):  # train ít epoch để demo\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output_dim = output.shape[-1]\n",
    "        loss = criterion(output[:,1:].reshape(-1, output_dim), tgt[:,1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, loss={total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84aab95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: (CNN)Share, and your gift will be multiplied. That may sound like an esoteric adage, but when Zully Broussard selflessly decided to give one of her kidneys to a stranger, her generosity paired up with big data. It resulted in six patients receiving transplants. That surprised and wowed her. \"I thought I was going to help this one person who I don't know, but the fact that so many people can have a ...\n",
      "Gold summary: Zully Broussard decided to give a kidney to a stranger .\n",
      "A new computer program helped her donation spur transplants for six kidney patients .\n",
      "Pred summary: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <unk> . <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> . <unk>\n"
     ]
    }
   ],
   "source": [
    "def summarize(model, text, max_len=30):\n",
    "    model.eval()\n",
    "    src = torch.tensor(encode(text, src_vocab, 100)).unsqueeze(0).to(device)\n",
    "    enc_outputs, hidden = model.encoder(src)\n",
    "    input_tok = torch.tensor([tgt_vocab[\"<sos>\"]]).to(device)\n",
    "    result = []\n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model.decoder(input_tok, hidden, enc_outputs)\n",
    "        top1 = output.argmax(1)\n",
    "        if top1.item() == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "        result.append(inv_tgt_vocab.get(top1.item(), \"<unk>\"))\n",
    "        input_tok = top1\n",
    "    return \" \".join(result)\n",
    "\n",
    "test_article = val_data[0][\"article\"]\n",
    "print(\"Article:\", test_article[:400], \"...\")\n",
    "print(\"Gold summary:\", val_data[0][\"highlights\"])\n",
    "print(\"Pred summary:\", summarize(model, test_article))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
