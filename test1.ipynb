{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4915a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.0233\n",
      "Epoch 40, Loss: 0.0095\n",
      "Epoch 60, Loss: 0.0054\n",
      "Epoch 80, Loss: 0.0035\n",
      "Epoch 100, Loss: 0.0025\n",
      "\n",
      "--- Demo inference ---\n",
      "Input: the cat sat on the mat\n",
      "Summary: cat on mat\n",
      "Input: dogs are playing in the park\n",
      "Summary: dogs playing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ======================\n",
    "# 1. Data toy example\n",
    "# ======================\n",
    "# Giả sử ta có vài cặp (input_text, summary) rất ngắn\n",
    "pairs = [\n",
    "    (\"the cat sat on the mat\", \"cat on mat\"),\n",
    "    (\"dogs are playing in the park\", \"dogs playing\"),\n",
    "    (\"a man is eating food\", \"man eating\"),\n",
    "    (\"a woman is reading a book\", \"woman reading\"),\n",
    "]\n",
    "\n",
    "# Xây vocab đơn giản\n",
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter()\n",
    "for src, tgt in pairs:\n",
    "    word_counts.update(src.split())\n",
    "    word_counts.update(tgt.split())\n",
    "\n",
    "# Tạo vocab\n",
    "vocab = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"] + sorted(word_counts.keys())\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "\n",
    "def encode(sentence):\n",
    "    return [word2idx.get(w, word2idx[\"<UNK>\"]) for w in sentence.split()]\n",
    "\n",
    "def decode(indices):\n",
    "    words = [idx2word[i] for i in indices if i not in (word2idx[\"<PAD>\"], word2idx[\"<SOS>\"], word2idx[\"<EOS>\"])]\n",
    "    return \" \".join(words)\n",
    "\n",
    "data = []\n",
    "for src, tgt in pairs:\n",
    "    src_ids = encode(src)\n",
    "    tgt_ids = [word2idx[\"<SOS>\"]] + encode(tgt) + [word2idx[\"<EOS>\"]]\n",
    "    data.append((src_ids, tgt_ids))\n",
    "\n",
    "# Pad function\n",
    "def pad(seq, max_len):\n",
    "    return seq + [word2idx[\"<PAD>\"]] * (max_len - len(seq))\n",
    "\n",
    "# ======================\n",
    "# 2. Model components\n",
    "# ======================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        outputs, (h, c) = self.lstm(embed)\n",
    "        return h, c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        embed = self.embedding(x.unsqueeze(1))\n",
    "        output, (h, c) = self.lstm(embed, (h, c))\n",
    "        logits = self.fc(output.squeeze(1))\n",
    "        return logits, h, c\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(self.device)\n",
    "        h, c = self.encoder(src)\n",
    "\n",
    "        input_token = tgt[:, 0]  # <SOS>\n",
    "        for t in range(1, tgt_len):\n",
    "            output, h, c = self.decoder(input_token, h, c)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# ======================\n",
    "# 3. Training\n",
    "# ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "EMB_DIM = 32\n",
    "HID_DIM = 64\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<PAD>\"])\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in data:\n",
    "        src_tensor = torch.tensor([pad(src, max_len=6)], dtype=torch.long).to(device)\n",
    "        tgt_tensor = torch.tensor([pad(tgt, max_len=6)], dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_tensor, tgt_tensor)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        tgt = tgt_tensor[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# ======================\n",
    "# 4. Inference\n",
    "# ======================\n",
    "def summarize(sentence, max_len=6):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = torch.tensor([pad(encode(sentence), max_len)], dtype=torch.long).to(device)\n",
    "        h, c = model.encoder(src)\n",
    "        input_token = torch.tensor([word2idx[\"<SOS>\"]], dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = []\n",
    "        for _ in range(max_len):\n",
    "            output, h, c = model.decoder(input_token, h, c)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == word2idx[\"<EOS>\"]:\n",
    "                break\n",
    "            outputs.append(top1.item())\n",
    "            input_token = top1\n",
    "    return decode(outputs)\n",
    "\n",
    "print(\"\\n--- Demo inference ---\")\n",
    "print(\"Input: the cat sat on the mat\")\n",
    "print(\"Summary:\", summarize(\"the cat sat on the mat\"))\n",
    "print(\"Input: dogs are playing in the park\")\n",
    "print(\"Summary:\", summarize(\"dogs are playing in the park\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
