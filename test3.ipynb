{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e311f417",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load CNN/DailyMail (phiên bản non-anonymized, thường dùng)\u001b[39;00m\n\u001b[32m      5\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mcnn_dailymail\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m3.0.0\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load CNN/DailyMail (phiên bản non-anonymized, thường dùng)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da11530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "max_input_len = 512\n",
    "max_target_len = 128\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"article\"], max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(batch[\"highlights\"], max_length=max_target_len, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess, batched=True, remove_columns=[\"article\",\"highlights\",\"id\"])\n",
    "encoded_dataset.set_format(type=\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,   # nhỏ vì Colab RAM ít\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].select(range(5000)),  # train nhỏ cho nhanh\n",
    "    eval_dataset=encoded_dataset[\"validation\"].select(range(500)),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = dataset[\"test\"][0][\"article\"]\n",
    "inputs = tokenizer([test_text], max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
    "\n",
    "print(\"Original:\", test_text[:500], \"...\")\n",
    "print(\"Gold Summary:\", dataset[\"test\"][0][\"highlights\"])\n",
    "print(\"Pred Summary:\", tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
