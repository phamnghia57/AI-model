1. Ki·∫øn tr√∫c t·ªïng qu√°t

M√¥ h√¨nh abstractive summarization = Encoder ‚Äì Decoder:

Encoder: ƒë·ªçc vƒÉn b·∫£n ƒë·∫ßu v√†o (paper text ‚Üí sequence of tokens).

Decoder: sinh ra summary (chu·ªói token ng·∫Øn g·ªçn h∆°n).

Attention: gi√∫p decoder t·∫≠p trung v√†o c√°c ph·∫ßn quan tr·ªçng trong vƒÉn b·∫£n.

üìå M·ªôt ki·∫øn tr√∫c kinh ƒëi·ªÉn:

Encoder = LSTM/GRU

Decoder = LSTM + Attention

(Tu·ª≥ ch·ªçn n√¢ng cao: Transformer Encoder‚ÄìDecoder)

2. D·ªØ li·ªáu

B·∫°n c·∫ßn dataset c√≥ (input text, summary). M·ªôt s·ªë l·ª±a ch·ªçn:

CNN/DailyMail dataset (tin t·ª©c + highlights).

XSum (BBC news + one-sentence summary).

ArXiv / PubMed dataset (t√≥m t·∫Øt b√†i b√°o khoa h·ªçc).

N·∫øu kh√¥ng mu·ªën dataset l·ªõn ‚Üí b·∫°n t·ª± l√†m dataset nh·ªè t·ª´ m·ªôt v√†i paper (copy ph·∫ßn abstract l√†m summary).

3. Pipeline hu·∫•n luy·ªán
B∆∞·ªõc 1. Ti·ªÅn x·ª≠ l√Ω

Tokenize input & output.

T·∫°o vocabulary (word2index, index2word).

Padding + truncation cho chu·ªói d√†i.

B∆∞·ªõc 2. X√¢y model Seq2Seq

V√≠ d·ª• v·ªõi PyTorch:

import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)

    def forward(self, x):
        embed = self.embedding(x)
        outputs, (h, c) = self.lstm(embed)
        return outputs, (h, c)


class Attention(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attn = nn.Linear(hidden_size*2, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        # hidden: (batch, hidden_size)
        # encoder_outputs: (batch, seq_len, hidden_size)
        seq_len = encoder_outputs.size(1)
        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        attention = self.v(energy).squeeze(2)
        return torch.softmax(attention, dim=1)


class Decoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size + hidden_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size*2, vocab_size)
        self.attention = Attention(hidden_size)

    def forward(self, x, hidden, cell, encoder_outputs):
        x = x.unsqueeze(1)
        embed = self.embedding(x)
        attn_weights = self.attention(hidden[-1], encoder_outputs)
        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)
        lstm_input = torch.cat((embed, attn_applied), dim=2)
        outputs, (h, c) = self.lstm(lstm_input, (hidden, cell))
        out = self.fc(torch.cat((outputs.squeeze(1), attn_applied.squeeze(1)), dim=1))
        return out, h, c, attn_weights

4. Hu·∫•n luy·ªán

Loss: nn.CrossEntropyLoss(ignore_index=PAD_IDX).

Optimizer: Adam.

Teacher forcing trong training (cho decoder th·∫•y ground-truth token).

Hu·∫•n luy·ªán v√†i epoch (dataset nh·ªè).

5. Sinh t√≥m t·∫Øt (Inference)

Cho input v√†o Encoder ‚Üí l·∫•y hidden states.

Decoder sinh token t·ª´ng b∆∞·ªõc (greedy search ho·∫∑c beam search).

D·ª´ng khi g·∫∑p <EOS>.

6. ƒê√°nh gi√°

Metric ph·ªï bi·∫øn: ROUGE (ROUGE-1, ROUGE-2, ROUGE-L).

So s√°nh summary sinh ra v·ªõi reference.

7. T·ªëi gi·∫£n cho mini-project

N·∫øu th·ªùi gian ng·∫Øn, b·∫°n c√≥ th·ªÉ:

Train tr√™n m·ªôt subset nh·ªè (vd: 10k samples t·ª´ CNN/DailyMail).

Gi·ªõi h·∫°n vocab (~20k words).

Tr√¨nh b√†y r√µ ki·∫øn tr√∫c Seq2Seq + Attention.

Demo inference tr√™n 1‚Äì2 paper th·ª±c t·∫ø.