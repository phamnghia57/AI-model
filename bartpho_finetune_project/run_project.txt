bartpho_finetune_project/
├─ data/
│  └─ data.csv
├─ scripts/
│  ├─ prepare_data.py
│  ├─ train.py
│  ├─ evaluate_model.py
│  └─ infer.py
├─ configs/
│  └─ training_config.yaml
├─ outputs/
│  └─ checkpoint-.../
└─ requirements.txt

#Open cmd -> cd '/thu muc project/'

python -m venv bartpho_env
bartpho_env\Scripts\activate
pip install -r requirements.txt
python scripts/prepapre_data.py
python scripts/train.py vinai/bartpho-word data/processed outputs/bartpho-finetuned
python scripts/evaluate_model.py outputs/bartpho-finetuned data/processed/test.csv
python scripts/infer.py outputs/bartpho-finetuned "TEXT"

# Tai mo hinh de su dung lai
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

model_dir = "outputs/bartpho-finetuned"  # thư mục đã lưu

tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Ví dụ sinh tóm tắt
text = "Hôm nay trời đẹp và học sinh tham gia ngày hội thể thao sôi nổi."
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
summary_ids = model.generate(inputs["input_ids"], attention_mask=inputs["attention_mask"], max_length=256, num_beams=4)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("Tóm tắt:", summary)


