import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import docx
import PyPDF2
import re
import requests
from newspaper import Article
from pathlib import Path
import time

# C√°c tham s·ªë c·ªë ƒë·ªãnh

DEFAULT_MODEL_DIR = "outputs/bartpho-finetuned"

# File test m·∫∑c ƒë·ªãnh trong m√¥ tr∆∞·ªùng
DEFAULT_TEST_DOCX = "/mnt/data/baocaohocmay.docx"

st.set_page_config(page_title="BartPho Summarizer ‚Äî Clean UI", layout="wide")


# H√†m h·ªó tr·ª£ ƒë·ªçc file pdf
def read_pdf(file_obj):
    """
    ƒê·ªçc v√† tr√≠ch xu·∫•t n·ªôi dung t·ª´ file PDF.

    Parameters
    ----------
    file_obj : UploadedFile
        File PDF ƒë∆∞·ª£c upload qua Streamlit.

    Returns
    -------
    str
        N·ªôi dung text ƒë√£ ƒë∆∞·ª£c gh√©p t·ª´ c√°c trang PDF.
    """
    reader = PyPDF2.PdfReader(file_obj)
    text = []
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text.append(page_text)
    return "\n".join(text)

# H√†m h·ªó tr·ª£ ƒë·ªçc file docx
def read_docx(path_or_file):
    """
    ƒê·ªçc n·ªôi dung t·ª´ file DOCX.  
    H·ªó tr·ª£ c·∫£ ƒë∆∞·ªùng d·∫´n file v√† file upload t·ª´ Streamlit.

    Parameters
    ----------
    path_or_file : str or UploadedFile
        ƒê∆∞·ªùng d·∫´n file ho·∫∑c ƒë·ªëi t∆∞·ª£ng file upload.

    Returns
    -------
    str
        N·ªôi dung vƒÉn b·∫£n thu·∫ßn (text) t·ª´ file DOCX.
    """
    if hasattr(path_or_file, "read"):
        doc = docx.Document(path_or_file)
    else:
        doc = docx.Document(str(path_or_file))
    return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])


def clean_text(text: str) -> str:
    """
    L√†m s·∫°ch vƒÉn b·∫£n: lo·∫°i b·ªè tab, kho·∫£ng tr·∫Øng th·ª´a, newline th·ª´a.

    Parameters
    ----------
    text : str
        VƒÉn b·∫£n c·∫ßn l√†m s·∫°ch.

    Returns
    -------
    str
        VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a.
    """
    if not text:
        return ""
    text = text.strip()
    text = re.sub(r"\t+", " ", text)
    text = re.sub(r" *\n+ *", "\n", text)
    text = re.sub(r" {2,}", " ", text)
    return text


# Load model & tokenizer v·ªõi cache ƒë·ªÉ tr√°nh load l·∫°i nhi·ªÅu l·∫ßn
@st.cache_resource
def load_model_and_tokenizer(model_dir: str):
    """
    Load tokenizer v√† m√¥ h√¨nh t√≥m t·∫Øt t·ª´ th∆∞ m·ª•c local (ƒë√£ fine-tune).

    Parameters
    ----------
    model_dir : str
        Th∆∞ m·ª•c ch·ª©a model v√† tokenizer.

    Returns
    -------
    tuple
        (tokenizer, model, device) ƒë√£ ƒë∆∞·ª£c load v√† ƒë∆∞a l√™n GPU n·∫øu c√≥.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    return tokenizer, model, device


# Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n theo token limit
def chunk_text_by_tokens(text: str, tokenizer, max_tokens: int = 800, overlap: int = 64):
    """
    Chia vƒÉn b·∫£n d√†i th√†nh nhi·ªÅu chunk d·ª±a tr√™n s·ªë l∆∞·ª£ng token.

    Parameters
    ----------
    text : str
        N·ªôi dung vƒÉn b·∫£n g·ªëc.
    tokenizer : AutoTokenizer
        Tokenizer ƒë·ªÉ encode text th√†nh token IDs.
    max_tokens : int
        S·ªë token t·ªëi ƒëa cho m·ªói chunk.
    overlap : int
        S·ªë token l·∫∑p l·∫°i gi·ªØa c√°c chunk (gi√∫p gi·ªØ ng·ªØ c·∫£nh).

    Returns
    -------
    list[str]
        Danh s√°ch c√°c chunk ƒë√£ t√°ch.
    """
    ids = tokenizer.encode(text)
    if len(ids) <= max_tokens:
        return [text]
    chunks = []
    start = 0
    while start < len(ids):
        end = start + max_tokens
        chunk_ids = ids[start:end]
        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        chunk_text = clean_text(chunk_text)
        if chunk_text:
            chunks.append(chunk_text)
        start = max(0, end - overlap)
        if end >= len(ids):
            break
    return chunks


# H√†m t√≥m t·∫Øt t·ª´ng chunk
def summarize_chunk(chunk: str, tokenizer, model, device, max_summary_tokens=128, num_beams=4):
    """
    T√≥m t·∫Øt m·ªôt chunk n·ªôi dung.

    Parameters
    ----------
    chunk : str
        Ph·∫ßn vƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt.
    tokenizer : AutoTokenizer
        Tokenizer ƒë√£ load.
    model : AutoModelForSeq2SeqLM
        M√¥ h√¨nh ƒë√£ load.
    device : str
        'cpu' ho·∫∑c 'cuda'.
    max_summary_tokens : int
        ƒê·ªô d√†i t·ªëi ƒëa c·ªßa b·∫£n t√≥m t·∫Øt.
    num_beams : int
        Beam search width.

    Returns
    -------
    str
        VƒÉn b·∫£n t√≥m t·∫Øt ƒë√£ l√†m s·∫°ch.
    """
    inputs = tokenizer(chunk, return_tensors="pt", truncation=True, max_length=1024)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs.get("attention_mask", None)
    gen_kwargs = dict(max_length=max_summary_tokens, num_beams=num_beams, early_stopping=True)
    if attention_mask is not None:
        gen = model.generate(input_ids=input_ids, attention_mask=attention_mask.to(device), **gen_kwargs)
    else:
        gen = model.generate(input_ids=input_ids, **gen_kwargs)
    summary = tokenizer.decode(gen[0], skip_special_tokens=True)
    return clean_text(summary)


# L·∫•y n·ªôi dung t·ª´ url
def extract_text_from_url(url: str, timeout: int = 10):
    """
    L·∫•y n·ªôi dung text t·ª´ URL qua Newspaper3k, fallback sang HTML parsing th√¥.

    Parameters
    ----------
    url : str
        ƒê·ªãa ch·ªâ trang web.
    timeout : int
        Timeout t·∫£i n·ªôi dung.

    Returns
    -------
    str
        N·ªôi dung text ƒë√£ l√†m s·∫°ch.

    Raises
    ------
    RuntimeError
        N·∫øu kh√¥ng th·ªÉ t·∫£i n·ªôi dung t·ª´ URL.
    """
    try:
        article = Article(url)
        article.download()
        article.parse()
        txt = clean_text(article.text)
        if txt and len(txt) > 50:
            return txt
    except Exception:
        pass
    # fallback: b√≥c html th√¥
    try:
        r = requests.get(url, timeout=timeout)
        html = r.text
        text = re.sub(r"<script.*?>.*?</script>", "", html, flags=re.S)
        text = re.sub(r"<style.*?>.*?</style>", "", text, flags=re.S)
        text = re.sub(r"<[^>]+>", " ", text)
        return clean_text(text)
    except Exception as e:
        raise RuntimeError(f"Kh√¥ng th·ªÉ t·∫£i n·ªôi dung t·ª´ URL: {e}")


# Layout ch√≠nh c·ªßa ·ª©ng d·ª•ng Streamlit
with st.sidebar:
    st.header("C·∫•u h√¨nh t√≥m t·∫Øt")
    model_dir = st.text_input("Model directory", value=DEFAULT_MODEL_DIR)
    max_chunk_tokens = st.number_input("Max chunk tokens", min_value=128, max_value=2048, value=800, step=64)
    chunk_overlap = st.number_input("Chunk overlap (tokens)", min_value=0, max_value=512, value=64, step=16)
    max_summary_tokens = st.number_input("Max summary tokens per chunk", min_value=32, max_value=512, value=128, step=16)
    num_beams = st.slider("Beams (num_beams)", 1, 8, 4)
    run_button = st.button("(Re)Load model")
    st.markdown("---")
    st.markdown("V√≠ d·ª•: b·∫°n ƒë√£ upload 1 file test: **/mnt/data/baocaohocmay.docx**")

# load model n·∫øu l·∫ßn ƒë·∫ßu ho·∫∑c khi nh·∫•n reload
if "model_loaded" not in st.session_state or run_button:
    try:
        with st.spinner("ƒêang load tokenizer & model (c√≥ th·ªÉ m·∫•t v√†i gi√¢y)..."):
            tokenizer, model, device = load_model_and_tokenizer(model_dir)
        st.session_state.tokenizer = tokenizer
        st.session_state.model = model
        st.session_state.device = device
        st.session_state.model_loaded = True
        st.success("Model loaded ‚úî")
    except Exception as e:
        st.error(f"L·ªói khi load model: {e}")
        st.stop()
else:
    tokenizer = st.session_state.tokenizer
    model = st.session_state.model
    device = st.session_state.device

# Upload file ho·∫∑c nh·∫≠p url
st.title("BartPho ‚Äî Summarizer (refined)")
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("1) Nh·∫≠p n·ªôi dung")
    uploaded = st.file_uploader("Upload PDF / DOCX (ho·∫∑c ƒë·ªÉ tr·ªëng d√πng file m·∫´u)", type=["pdf", "docx"], accept_multiple_files=False)
    url_input = st.text_input("Ho·∫∑c nh·∫≠p URL b√†i b√°o ƒë·ªÉ t√≥m t·∫Øt")
    use_sample = st.checkbox("S·ª≠ d·ª•ng file m·∫´u (/mnt/data/baocaohocmay.docx)", value=False)

with col2:
    st.subheader("2) T√πy ch·ªçn nhanh")
    show_orig = st.checkbox("Hi·ªán n·ªôi dung g·ªëc (c·∫Øt ng·∫Øn)", value=True)
    show_chunks = st.checkbox("Hi·ªán c√°c chunk", value=False)
    show_progress = st.checkbox("Hi·ªán progress bar", value=True)

# Thu th·∫≠p n·ªôi dung t·ª´ ngu·ªìn t∆∞∆°ng ·ª©ng
text = ""
source_label = ""
if uploaded:
    source_label = uploaded.name
    if uploaded.type == "application/pdf":
        text = read_pdf(uploaded)
    else:
        text = read_docx(uploaded)
elif url_input:
    source_label = url_input
    try:
        with st.spinner("ƒêang t·∫£i n·ªôi dung t·ª´ URL ..."):
            text = extract_text_from_url(url_input)
    except Exception as e:
        st.error(str(e))
        st.stop()
elif use_sample:
    path = Path(DEFAULT_TEST_DOCX)
    if path.exists():
        source_label = str(path)
        text = read_docx(path)
    else:
        st.error(f"File m·∫´u kh√¥ng t·ªìn t·∫°i: {path}")
        st.stop()
else:
    st.info("Vui l√≤ng upload file ho·∫∑c nh·∫≠p URL ho·∫∑c ch·ªçn file m·∫´u ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

if not text:
    st.stop()

text = clean_text(text)

# Hi·ªÉn th·ªã n·ªôi dung ngu·ªìn
if show_orig:
    st.subheader(f"N·ªôi dung ngu·ªìn ‚Äî {source_label}")
    st.write(text[:3000] + (" ..." if len(text) > 3000 else ""))

# chunking
chunks = chunk_text_by_tokens(text, tokenizer, max_tokens=int(max_chunk_tokens), overlap=int(chunk_overlap))

st.write(f"üîπ T·ªïng tokens (∆∞·ªõc l∆∞·ª£ng): {len(tokenizer.encode(text))} ‚Äî S·ªë chunk: {len(chunks)}")
if show_chunks:
    for i, c in enumerate(chunks, 1):
        st.markdown(f"**Chunk {i}** ‚Äî ({len(tokenizer.encode(c))} tokens)")
        st.write(c[:1000] + (" ..." if len(c) > 1000 else ""))

# Summarize button
if st.button("T√≥m t·∫Øt now ‚Äî Summarize"):
    final = []
    progress = st.progress(0)
    total = len(chunks)
    for i, ch in enumerate(chunks, 1):
        if show_progress:
            st.write(f"‚è≥ T√≥m t·∫Øt chunk {i}/{total} ‚Äî tokens {len(tokenizer.encode(ch))}")
        try:
            s = summarize_chunk(ch, tokenizer, model, device, max_summary_tokens=int(max_summary_tokens), num_beams=int(num_beams))
        except Exception as e:
            s = f"‚ö† L·ªói khi t√≥m t·∫Øt chunk {i}: {e}"
        final.append(s)
        if show_progress:
            progress.progress(int(i/total*100))
    joined = "\n\n".join(final)
    st.subheader("K·∫øt qu·∫£ t√≥m t·∫Øt (t·ª´ng chunk gh√©p l·∫°i)")
    st.write(clean_text(joined))
    # Final short summary
    if len(tokenizer.encode(joined)) > 32:
        with st.expander("T·∫°o t√≥m t·∫Øt ng·∫Øn g·ªçn t·ª´ b·∫£n t√≥m t·∫Øt" , expanded=False):
            try:
                short = summarize_chunk(joined, tokenizer, model, device, max_summary_tokens=200, num_beams=4)
                st.markdown("**T√≥m t·∫Øt ng·∫Øn g·ªçn:**")
                st.write(short)
            except Exception as e:
                st.write(f"Kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt ng·∫Øn h∆°n: {e}")


# Footer
st.markdown("---")
st.caption("·ª®ng d·ª•ng ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ ch·∫°y v·ªõi m√¥ h√¨nh fine-tuned t·∫°i local. C·∫ßn ƒë·∫£m b·∫£o model v√† tokenizer n·∫±m trong th∆∞ m·ª•c c·∫•u h√¨nh.")
